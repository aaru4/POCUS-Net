{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install anvil-uplink"
      ],
      "metadata": {
        "id": "DMEqBWDjz1sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anvil.server\n",
        "anvil.server.connect(\"server_HAQPFOVYCD4RPFILZ4SIBZY2-VOEAFQ7CLLD47XM6\")"
      ],
      "metadata": {
        "id": "M2qWjCLJz6Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "SgMGoRSZ4WMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908a7729-552a-459e-fd12-3b6c31469ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
      ],
      "metadata": {
        "id": "KMbcaFE3IwQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "folder_name = 'CVAT'\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, os.path.join(folder_name, filename))\n"
      ],
      "metadata": {
        "id": "WSZZSmIvm9cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwHZC4hSYT2A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import xml.etree.ElementTree as ET  # For parsing XML\n",
        "import zipfile\n",
        "\n",
        "# Define  categories\n",
        "categories = [\"empty\", \"clear_fluid\", \"solid\", \"air\"]\n",
        "\n",
        "image_dir = \"/content/CVAT\"\n",
        "image_filenames = os.listdir(image_dir)\n",
        "\n",
        "# Define a list to store labels for each image\n",
        "labels = []\n",
        "\n",
        "for filename in image_filenames:\n",
        "    if \"empty\" in filename or \"air\" in filename:\n",
        "        label = [1, 0, 0, 0]  # Empty antrum\n",
        "    elif \"fluid\" in filename:\n",
        "        label = [0, 1, 0, 0]  # Clear fluid in the antrum\n",
        "    elif \"solid\" in filename:\n",
        "        label = [0, 0, 1, 0]  # Solids in the antrum\n",
        "\n",
        "    labels.append(label)\n",
        "\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(labels)\n",
        "\n",
        "'''\n",
        "# Print the labels and an image\n",
        "for i, filename in enumerate(image_filenames):\n",
        "    # Skip directories\n",
        "    if os.path.isdir(os.path.join(image_dir, filename)):\n",
        "        continue\n",
        "\n",
        "    print(\"Image: \", filename)\n",
        "    print(\"Label: \", labels[i])\n",
        "\n",
        "    # Load and display the image\n",
        "    image_path = os.path.join(image_dir, filename)\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "labeling and preprocessing images"
      ],
      "metadata": {
        "id": "lDGACqVe-O-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import xml.etree.ElementTree as ET  # For parsing XML\n",
        "import zipfile\n",
        "\n",
        "# Define  categories\n",
        "categories = [\"empty\", \"clear_fluid\", \"solid\", \"air\"]\n",
        "\n",
        "zip_file_path = \"/content/segmentantrum11.29.zip\"\n",
        "extracted_folder_path = \"/content/extracted_annotations\"\n",
        "\n",
        "# Extract zip file to a folder\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "# List the files in the extracted folder\n",
        "extracted_filenames = os.listdir(extracted_folder_path)\n",
        "image_dir = \"/content/CVAT\"\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "xml_file_path = \"/content/extracted_annotations/annotations.xml\"\n",
        "output_dir = \"/content/extracted_annotations/indiv_xml\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Parse xml file\n",
        "tree = ET.parse(xml_file_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "# Iterate through the <image> xml elements\n",
        "for image_elem in root.findall(\".//image\"):\n",
        "    # Extract image attributes\n",
        "    image_name = image_elem.get(\"name\")\n",
        "\n",
        "    # Create a new XML file for this image\n",
        "    image_xml = ET.Element(\"annotations\")\n",
        "    version_elem = ET.SubElement(image_xml, \"version\")\n",
        "    version_elem.text = \"1.1\"\n",
        "    meta_elem = ET.SubElement(image_xml, \"meta\")\n",
        "    image_elem_copy = ET.Element(\"image\")\n",
        "    image_elem_copy.set(\"name\", image_name)\n",
        "    meta_elem.append(image_elem_copy)\n",
        "\n",
        "    # Find the annotations for this image\n",
        "    annotations = root.findall(f\".//image[@name='{image_name}']/*\")\n",
        "\n",
        "    # Add the annotations for this image to the new XML file\n",
        "    for annotation in annotations:\n",
        "        image_xml.append(annotation)\n",
        "\n",
        "    # Save the new XML file with the same name as the image\n",
        "    output_xml_path = os.path.join(output_dir, image_name.replace(\" \", \"_\").replace(\".\", \"_\") + \".xml\")\n",
        "    tree = ET.ElementTree(image_xml)\n",
        "    tree.write(output_xml_path)\n",
        "\n",
        "print(\"Individual XML files have been created in:\", output_dir)\n"
      ],
      "metadata": {
        "id": "8cZmniy-7wKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "simulation of ultrasound code, not run each time"
      ],
      "metadata": {
        "id": "BSKOARXlOMpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Reshape, Conv2DTranspose, Conv2D, Flatten, BatchNormalization, LeakyReLU\n",
        "\n",
        "image_dir = \"/content/CVAT\"\n",
        "def load_images_from_directory(image_dir):\n",
        "    image_list = []\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            img_path = os.path.join(image_dir, filename)\n",
        "            img = Image.open(img_path)\n",
        "            img = img.convert('L')  # Grayscale\n",
        "            img = img.resize((28, 28))  # Resize to match input shape\n",
        "            img_array = np.array(img)\n",
        "            img_array = img_array / 255.0  # Normalize pixel values\n",
        "            img_array = (img_array - 0.5) / 0.5  # Standardize pixel values\n",
        "            img_array = np.expand_dims(img_array, axis=-1)  # Add dimension\n",
        "            image_list.append(img_array)\n",
        "    return np.array(image_list)\n",
        "\n",
        "def build_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(7 * 7 * 256, input_dim=latent_dim))\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh'))\n",
        "    return model\n",
        "\n",
        "def build_discriminator(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "def build_progan(latent_dim, input_shape):\n",
        "    generator = build_generator(latent_dim)\n",
        "    discriminator = build_discriminator(input_shape)\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    discriminator.trainable = False\n",
        "    gan = Sequential()\n",
        "    gan.add(generator)\n",
        "    gan.add(discriminator)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "def generate_and_print_images(generator, num_images):\n",
        "    latent_vectors = np.random.randn(num_images, latent_dim)\n",
        "    generated_images = generator.predict(latent_vectors)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Set hyperparameters\n",
        "latent_dim = 100\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Build the ProGAN model\n",
        "progan = build_progan(latent_dim, input_shape)\n",
        "\n",
        "# Generate and print 9 images\n",
        "generate_and_print_images(progan.layers[0], 9)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "8FB3vnsKOO5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "merging antrum annotations and images"
      ],
      "metadata": {
        "id": "63w1Nvqi-SNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from IPython.display import Image, display\n",
        "\n",
        "image_dir = \"/content/CVAT\n",
        "xml_dir = \"/content/extracted_annotations/indiv_xml\"\n",
        "output_dir = \"/content/outputtedCVAT\"\n",
        "\n",
        "# Create the output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Iterate through the XML annotation files\n",
        "for xml_file in os.listdir(xml_dir):\n",
        "    if xml_file.endswith(\".xml\"):\n",
        "        xml_path = os.path.join(xml_dir, xml_file)\n",
        "\n",
        "        # Determine the image format\n",
        "        image_name_without_extension = os.path.splitext(xml_file)[0]\n",
        "        jpg_image_path = os.path.join(image_dir, image_name_without_extension + \".jpg\")\n",
        "        png_image_path = os.path.join(image_dir, image_name_without_extension + \".png\")\n",
        "\n",
        "        if os.path.exists(jpg_image_path):\n",
        "            image_path = jpg_image_path\n",
        "        elif os.path.exists(png_image_path):\n",
        "            image_path = png_image_path\n",
        "        else:\n",
        "            print(f\"Image not found for XML: {xml_file}\")\n",
        "            continue  # Skip this XML file if the image is not found\n",
        "\n",
        "        # Load the image\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        # Check if the image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Failed to load image: {image_path}\")\n",
        "            continue  # Skip this image and move to the next one\n",
        "\n",
        "        # Parse the XML file\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        for annotation_elem in root:\n",
        "            if annotation_elem.tag == \"ellipse\":\n",
        "                # Extract ellipse attributes\n",
        "                cx = int(float(annotation_elem.get(\"cx\")))\n",
        "                cy = int(float(annotation_elem.get(\"cy\")))\n",
        "                rx = int(float(annotation_elem.get(\"rx\")))\n",
        "                ry = int(float(annotation_elem.get(\"ry\")))\n",
        "                label = annotation_elem.get(\"label\")\n",
        "\n",
        "                # Draw an ellipse on the image\n",
        "                cv2.ellipse(img, (cx, cy), (rx, ry), 0, 0, 360, (0, 0, 255), 2)\n",
        "                cv2.putText(img, label, (cx - 20, cy - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "            elif annotation_elem.tag == \"polyline\":\n",
        "                # Extract polyline attributes\n",
        "                points_str = annotation_elem.get(\"points\")\n",
        "                points = [int(float(coord.split(',')[0])) for coord in points_str.split(';')], \\\n",
        "                         [int(float(coord.split(',')[1])) for coord in points_str.split(';')]\n",
        "                points = [(points[0][i], points[1][i]) for i in range(len(points[0]))]\n",
        "\n",
        "                # Draw the polyline on the image\n",
        "                cv2.polylines(img, [np.array(points, dtype=np.int32)], isClosed=False, color=(0, 0, 255), thickness=2)\n",
        "\n",
        "                label = annotation_elem.get(\"label\")\n",
        "                if label:\n",
        "                    label_x = points[0][0] - 20\n",
        "                    label_y = points[1][0] - 20\n",
        "                    cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Save the annotated image\n",
        "        annotated_image_path = os.path.join(output_dir, image_name_without_extension + \".jpg\")\n",
        "        cv2.imwrite(annotated_image_path, img)\n",
        "\n",
        "print(\"Annotated images have been saved in:\", output_dir)\n",
        "# Directory containing annotated images\n",
        "output_dir = \"/content/outputtedCVAT\"\n",
        "\n",
        "# List the files in the output directory\n",
        "annotated_image_files = os.listdir(output_dir)\n",
        "annotated_image_files = [file for file in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, file))]\n",
        "\n",
        "# Display the first few annotated images\n",
        "num_images_to_display = min(len(annotated_image_files), 15)\n",
        "for i in range(num_images_to_display):\n",
        "    image_path = os.path.join(output_dir, annotated_image_files[i])\n",
        "    display(Image(filename=image_path))\n"
      ],
      "metadata": {
        "id": "9P4KdWZn-U5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "id": "fbCkpnUe_21W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir foldername"
      ],
      "metadata": {
        "id": "9QOxuCNHY6Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT Adaptor model"
      ],
      "metadata": {
        "id": "6wstqTylYoYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageData\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset_dir = '/content/CVAT'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 128, 128\n",
        "batch_size = 8\n",
        "\n",
        "# Data preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load and augment the training dataset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',  # Change class_mode to 'categorical'\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Define the CNN architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(learning_rate=1e-4),  # Use learning_rate instead of lr\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# h5 is one of the Hierarchical Data Formats (HDF) used to store large amount of data\n",
        "history = model.fit(train_generator, epochs=24)\n",
        "model.save('gastric_ultrasound_model.h5')\n",
        "\n",
        "# Display some sample images from the dataset\n",
        "def show_images(generator, num_images=5):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        image = next(generator)[0]\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display some sample images from the dataset\n",
        "def show_images(generator, num_images=5):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    images, labels = next(generator)\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display sample images after training\n",
        "show_images(train_generator, num_images=5)\n"
      ],
      "metadata": {
        "id": "-exSG-TAUcEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import albumentations as A\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "np.random.seed(45)\n",
        "tf.random.set_seed(45)\n",
        "\n",
        "image_dir = \"/content/outputtedCVAT\"\n",
        "xml_dir = \"/content/extracted_annotations/indiv_xml\"\n",
        "output_dir = \"/content/mergedCVAT\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "data_dir = output_dir\n",
        "\n",
        "# Define batch size and number of epochs\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "\n",
        "albumentations_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "])\n",
        "\n",
        "# data generator using tf.data and Albumentations\n",
        "@tf.function\n",
        "def process_path(file_path):\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))  # Adjust the target size\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "    img_np = img.numpy()\n",
        "\n",
        "    img_np = albumentations_transform(image=img_np)['image']\n",
        "\n",
        "    return img_np\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0,  # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,   # Randomly rotate images by up to 20 degrees\n",
        "    width_shift_range=0.2,  # Randomly shift the width of images\n",
        "    height_shift_range=0.2,  # Randomly shift the height of images\n",
        "    horizontal_flip=True  # Randomly flip images horizontally\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,  # Path to the data directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=15\n",
        ")\n",
        "\n",
        "class SqueezeExciteBlock(layers.Layer):\n",
        "    def __init__(self, ratio=8, **kwargs):\n",
        "        super(SqueezeExciteBlock, self).__init__(**kwargs)\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.filters = input_shape[-1]\n",
        "        self.se_reduce = layers.Conv2D(self.filters // self.ratio, kernel_size=1, activation='relu', strides=1, padding='same')\n",
        "        self.se_expand = layers.Conv2D(self.filters, kernel_size=1, activation='sigmoid', strides=1, padding='same')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        se_tensor = layers.GlobalAveragePooling2D()(inputs)\n",
        "        se_tensor = tf.reshape(se_tensor, [-1, 1, 1, self.filters])\n",
        "        se_tensor = self.se_reduce(se_tensor)\n",
        "        se_tensor = self.se_expand(se_tensor)\n",
        "        return inputs * se_tensor\n",
        "\n",
        "class VisionTransformerAdapter(nn.Module):\n",
        "    def __init__(self, input_channels, d_model, hidden_size, dropout_rate=0.1):\n",
        "        super(VisionTransformerAdapter, self).__init__()\n",
        "\n",
        "        self.conv1d = nn.Conv1d(input_channels + d_model, hidden_size, kernel_size=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.feed_forward = nn.Linear(hidden_size, d_model)\n",
        "        self.conv2d = nn.Conv2d(1, hidden_size, kernel_size=3, stride=1, padding=1)\n",
        "        self.se_block = SqueezeExciteBlock(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, positional_embedding):\n",
        "        x = x + positional_embedding\n",
        "        x = x.view(x.size(0), x.size(1), -1).permute(0, 2, 1)\n",
        "        x = self.conv1d(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv2d(x)\n",
        "        x = self.se_block(x)  # Apply SE block\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "    def call(self, inputs):\n",
        "        se_tensor = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
        "        se_tensor = self._se_reduce(se_tensor)\n",
        "        se_tensor = tf.nn.relu(se_tensor)\n",
        "        se_tensor = self._se_expand(se_tensor)\n",
        "        se_tensor = tf.nn.sigmoid(se_tensor)\n",
        "        return inputs * se_tensor\n",
        "\n",
        "# Define the neural network architecture with Squeeze-and-Excitation\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    SqueezeExciteBlock(ratio=8),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.00001)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',  # categorical crossentropy for one-hot encoded labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Iterate through the XML annotation files\n",
        "for xml_file in os.listdir(xml_dir):\n",
        "    if xml_file.endswith(\".xml\"):\n",
        "        xml_path = os.path.join(xml_dir, xml_file)\n",
        "\n",
        "        # Determine the image format\n",
        "        image_name_without_extension = os.path.splitext(xml_file)[0]\n",
        "        jpg_image_path = os.path.join(image_dir, image_name_without_extension + \".jpg\")\n",
        "        png_image_path = os.path.join(image_dir, image_name_without_extension + \".png\")\n",
        "\n",
        "        if os.path.exists(jpg_image_path):\n",
        "            image_path = jpg_image_path\n",
        "        elif os.path.exists(png_image_path):\n",
        "            image_path = png_image_path\n",
        "        else:\n",
        "            print(f\"Image not found for XML: {xml_file}\")\n",
        "            continue  # Skip this XML file if the image is not found\n",
        "\n",
        "        # Load the image\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        # Check if the image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Failed to load image: {image_path}\")\n",
        "            continue  # Skip this image and move to the next one\n",
        "\n",
        "        # Parse the XML file\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        for annotation_elem in root:\n",
        "            if annotation_elem.tag == \"ellipse\":\n",
        "                # Extract ellipse attributes\n",
        "                cx = int(float(annotation_elem.get(\"cx\")))\n",
        "                cy = int(float(annotation_elem.get(\"cy\")))\n",
        "                rx = int(float(annotation_elem.get(\"rx\")))\n",
        "                ry = int(float(annotation_elem.get(\"ry\")))\n",
        "                label = annotation_elem.get(\"label\")\n",
        "\n",
        "                # Draw an ellipse on the image\n",
        "                cv2.ellipse(img, (cx, cy), (rx, ry), 0, 0, 360, (0, 0, 255), 2)\n",
        "                cv2.putText(img, label, (cx - 20, cy - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "            elif annotation_elem.tag == \"polyline\":\n",
        "                # Extract polyline attributes\n",
        "                points_str = annotation_elem.get(\"points\")\n",
        "                points = [int(float(coord.split(',')[0])) for coord in points_str.split(';')], \\\n",
        "                         [int(float(coord.split(',')[1])) for coord in points_str.split(';')]\n",
        "                points = [(points[0][i], points[1][i]) for i in range(len(points[0]))]\n",
        "\n",
        "                # Draw the polyline on the image\n",
        "                cv2.polylines(img, [np.array(points, dtype=np.int32)], isClosed=False, color=(0, 0, 255), thickness=2)\n",
        "\n",
        "                label = annotation_elem.get(\"label\")\n",
        "                if label:\n",
        "                    label_x = points[0][0] - 20\n",
        "                    label_y = points[1][0] - 20\n",
        "                    cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Save the annotated image\n",
        "        annotated_image_path = os.path.join(output_dir, image_name_without_extension + \".jpg\")\n",
        "        cv2.imwrite(annotated_image_path, img)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=epochs,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    data_dir,  # Path to the data directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "submitted_ultrasound_path = \"/content/submittedUltrasound.png\"\n",
        "submitted_ultrasound = cv2.imread(submitted_ultrasound_path)\n",
        "submitted_ultrasound = cv2.resize(submitted_ultrasound, (224, 224))\n",
        "submitted_ultrasound = np.expand_dims(submitted_ultrasound, axis=0) / 255.0\n",
        "\n",
        "# Make predictions on the submitted ultrasound image\n",
        "predictions = model.predict(submitted_ultrasound)\n",
        "\n",
        "# Convert predictions to class label\n",
        "predicted_class = np.argmax(predictions)\n",
        "print(f\"Predicted Class for Submitted Ultrasound: {predicted_class}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "2LuNk3F3ZPa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "diameter"
      ],
      "metadata": {
        "id": "XJ8w7X9IWMyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/diameterAnnotations.zip'\n",
        "extracted_dir = '/content/extractions'\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the specified directory\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "print(f\"Contents extracted to: {extracted_dir}\")"
      ],
      "metadata": {
        "id": "PRTR5vwDWD3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow  # Import cv2_imshow for displaying images in Colab\n",
        "\n",
        "# Function to parse the XML file and get annotations\n",
        "def parse_annotations(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for image_elem in root.findall('image'):\n",
        "        image_name = image_elem.get('name')\n",
        "        image_annotations = []\n",
        "\n",
        "        for polyline_elem in image_elem.findall('polyline'):\n",
        "            label = polyline_elem.get('label')\n",
        "            points_str = polyline_elem.get('points')\n",
        "            points = [tuple(map(float, point.split(','))) for point in points_str.split(';')]\n",
        "\n",
        "            image_annotations.append({\n",
        "                'label': label,\n",
        "                'points': points\n",
        "            })\n",
        "\n",
        "        annotations.append({\n",
        "            'image_name': image_name,\n",
        "            'annotations': image_annotations\n",
        "        })\n",
        "\n",
        "    return annotations\n",
        "\n",
        "# print annotated images\n",
        "def print_annotated_images(annotations, images_dir):\n",
        "    for annotation in annotations:\n",
        "        image_name = annotation['image_name']\n",
        "        image_path = os.path.join(images_dir, image_name)\n",
        "\n",
        "        # Load the image using OpenCV\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        if image is None:\n",
        "            print(f\"Error loading image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        for polyline in annotation['annotations']:\n",
        "            label = polyline['label']\n",
        "            points = np.array(polyline['points'], dtype=np.int32)  # Convert points to int32\n",
        "\n",
        "            # Draw polyline on the image\n",
        "            cv2.polylines(image, [points], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        # Display the image with annotations\n",
        "        cv2_imshow(image)\n",
        "\n",
        "xml_file_path = '/content/extractions/annotations.xml'\n",
        "images_dir = '/content/images'\n",
        "\n",
        "annotations = parse_annotations(xml_file_path)\n",
        "print_annotated_images(annotations, images_dir)"
      ],
      "metadata": {
        "id": "gCGEdDCqWGws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam  # Import Adam optimizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# parse the XML file and get annotations\n",
        "def parse_annotations(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for image_elem in root.findall('image'):\n",
        "        image_name = image_elem.get('name')\n",
        "        image_annotations = []\n",
        "\n",
        "        for polyline_elem in image_elem.findall('polyline'):\n",
        "            label = polyline_elem.get('label')\n",
        "            points_str = polyline_elem.get('points')\n",
        "            points = [tuple(map(float, point.split(','))) for point in points_str.split(';')]\n",
        "\n",
        "            image_annotations.append({\n",
        "                'label': label,\n",
        "                'points': points\n",
        "            })\n",
        "\n",
        "        annotations.append({\n",
        "            'image_name': image_name,\n",
        "            'annotations': image_annotations\n",
        "        })\n",
        "\n",
        "    return annotations\n",
        "\n",
        "# Load annotations\n",
        "xml_file_path = '/content/extractions/annotations.xml'\n",
        "images_dir = '/content/CVAT'\n",
        "\n",
        "annotations = parse_annotations(xml_file_path)\n",
        "\n",
        "X = []  # Images\n",
        "labels = []  # List of Labels\n",
        "\n",
        "for annotation in annotations:\n",
        "    image_name = annotation['image_name']\n",
        "    image_path = os.path.join(images_dir, image_name)\n",
        "\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    if image is None:\n",
        "        print(f\"Error loading image: {image_path}\")\n",
        "        continue\n",
        "\n",
        "    for polyline in annotation['annotations']:\n",
        "        label_value = polyline['label']  # Use a different variable name\n",
        "        points = np.array(polyline['points'], dtype=np.int32)\n",
        "\n",
        "        # Extract the region of interest (ROI) from the image based on the points\n",
        "        x, y, w, h = cv2.boundingRect(points)\n",
        "        roi = image[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize the ROI to a fixed size\n",
        "        roi = cv2.resize(roi, (224, 224))\n",
        "\n",
        "        X.append(roi)\n",
        "        labels.append(label_value)  # Append the label value\n",
        "\n",
        "X = np.array(X)\n",
        "labels = np.array(labels)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model with Adam optimizer and a learning rate of 0.001\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Set the learning rate for the Adam optimizer\n",
        "adam_optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=28, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the trained model\n",
        "model.save('diameter_detection_model.h5')\n",
        "\n",
        "# Load the submitted ultrasound image\n",
        "submitted_ultrasound_path = '/content/submittedUltrasound.png'\n",
        "submitted_ultrasound = cv2.imread(submitted_ultrasound_path)\n",
        "\n",
        "# Preprocess the submitted ultrasound image\n",
        "submitted_ultrasound = cv2.resize(submitted_ultrasound, (224, 224))\n",
        "\n",
        "# Expand dimensions to match the input shape expected by the model\n",
        "submitted_ultrasound = np.expand_dims(submitted_ultrasound, axis=0)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(submitted_ultrasound)\n",
        "\n",
        "# Decode the predicted labels using the label encoder\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Display or save the annotated image\n",
        "cv2.imshow('Annotated Ultrasound', submitted_ultrasound[0])\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "O3_N6bWaWJXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "access_diameters = input(\"Use your ultrasound's scale to determine the length of the AP and CC diameters above. Type proceed when complete.  \").lower()\n",
        "\n",
        "def calculate_gastric_volume():\n",
        "\n",
        "    age_input = input(\"Enter patient age: \")\n",
        "    ap_diameter_input = input(\"Measure the anteroposterior diameter of the antrum and enter value: \")\n",
        "    cc_diameter_input = input(\"Measure the craniocaudal diameter of the antrum and enter value: \")\n",
        "\n",
        "    try:\n",
        "        age = float(age_input)\n",
        "        ap_diameter = float(ap_diameter_input)\n",
        "        cc_diameter = float(cc_diameter_input)\n",
        "\n",
        "        # Calculate gastric volume\n",
        "        gastric_volume = 27 + (15.6 * (math.pi * ap_diameter * cc_diameter / 4)) - (1.28 * age)\n",
        "\n",
        "        return gastric_volume\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter numeric values.\")\n",
        "        return None\n",
        "\n",
        "if access_diameters:\n",
        "  gastric_volume = calculate_gastric_volume()\n"
      ],
      "metadata": {
        "id": "ZI__5NEVWKox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if predicted_class == 1 or predicted_class == 4:\n",
        "  print(\"Empty antrum. Proceed with operation.\")\n",
        "elif predicted_class == 2:\n",
        "  print(\"Clear fluid present in antrum.\")\n",
        "  if gastric_volume <= 1.5:\n",
        "    print(\"Gastric volume is <= 1.5ml/kg. Proceed with operation.\")\n",
        "  else:\n",
        "    print(\"Inoperable conditions. High risk of aspiration. If surgery is urgent or emergent, proceed with full stomach precaution.\")\n",
        "elif predicted_class == 3:\n",
        "  print(\"Inoperable conditions, undigested solids are present in stomach. High risk of aspiration. If surgery is urgent or emergent, proceed with full stomach precaution.\")\n"
      ],
      "metadata": {
        "id": "kXqfcBZzICYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}